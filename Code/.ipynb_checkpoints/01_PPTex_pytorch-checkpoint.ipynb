{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d87bea2-fe91-4ed9-84c1-823a2cb2c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e583b97-d15f-4620-bc57-743bd60ba0ac",
   "metadata": {},
   "source": [
    "### Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c67cc79-d8c4-477b-a787-c54b76c3d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1997e-10, 0.0000e+00])\n",
      "tensor([[1.1325e-10, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0.4423, 0.1775, 0.9793],\n",
      "        [0.8245, 0.6558, 0.8836]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.float32\n",
      "torch.int32\n",
      "torch.Size([2, 3])\n",
      "tensor([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2)\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(2, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(2, 3)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 3, dtype=torch.int)\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 3)\n",
    "print(x.size())\n",
    "\n",
    "x = torch.tensor([2, 3])\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc30ed2-3109-4a7f-8eb1-2f2aedbca003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5])\n",
      "tensor([2, 3])\n",
      "tensor([2, 3])\n",
      "tensor([4, 5])\n",
      "tensor([4, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "x.requires_grad_(False)\n",
    "print(x)\n",
    "\n",
    "y = x.detach()\n",
    "print(y)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843d3fbc-67ea-4148-aec0-11afd6564505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., requires_grad=True)\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "y.backward()\n",
    "\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a1d70c-a667-43b1-8884-f63ec4317226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True)\n",
      "epoch 1: w = 0.677, loss = 75.66666412\n",
      "epoch 2: w = 1.148, loss = 36.82336807\n",
      "epoch 3: w = 1.476, loss = 17.97099113\n",
      "epoch 4: w = 1.705, loss = 8.82109165\n",
      "epoch 5: w = 1.865, loss = 4.38023996\n",
      "epoch 6: w = 1.976, loss = 2.22489786\n",
      "epoch 7: w = 2.053, loss = 1.17881405\n",
      "epoch 8: w = 2.107, loss = 0.67110348\n",
      "epoch 9: w = 2.145, loss = 0.42468837\n",
      "epoch 10: w = 2.171, loss = 0.30509272\n",
      "epoch 11: w = 2.189, loss = 0.24704708\n",
      "epoch 12: w = 2.202, loss = 0.21887507\n",
      "epoch 13: w = 2.210, loss = 0.20520197\n",
      "epoch 14: w = 2.217, loss = 0.19856580\n",
      "epoch 15: w = 2.221, loss = 0.19534503\n",
      "epoch 16: w = 2.224, loss = 0.19378185\n",
      "epoch 17: w = 2.226, loss = 0.19302319\n",
      "epoch 18: w = 2.227, loss = 0.19265491\n",
      "epoch 19: w = 2.228, loss = 0.19247620\n",
      "epoch 20: w = 2.229, loss = 0.19238949\n",
      "epoch 21: w = 2.230, loss = 0.19234735\n",
      "epoch 22: w = 2.230, loss = 0.19232696\n",
      "epoch 23: w = 2.230, loss = 0.19231701\n",
      "epoch 24: w = 2.230, loss = 0.19231229\n",
      "epoch 25: w = 2.231, loss = 0.19230990\n",
      "epoch 26: w = 2.231, loss = 0.19230872\n",
      "epoch 27: w = 2.231, loss = 0.19230825\n",
      "epoch 28: w = 2.231, loss = 0.19230795\n",
      "epoch 29: w = 2.231, loss = 0.19230781\n",
      "epoch 30: w = 2.231, loss = 0.19230779\n",
      "epoch 31: w = 2.231, loss = 0.19230773\n",
      "epoch 32: w = 2.231, loss = 0.19230776\n",
      "epoch 33: w = 2.231, loss = 0.19230770\n",
      "epoch 34: w = 2.231, loss = 0.19230765\n",
      "epoch 35: w = 2.231, loss = 0.19230767\n",
      "epoch 36: w = 2.231, loss = 0.19230767\n",
      "epoch 37: w = 2.231, loss = 0.19230771\n",
      "epoch 38: w = 2.231, loss = 0.19230773\n",
      "epoch 39: w = 2.231, loss = 0.19230765\n",
      "epoch 40: w = 2.231, loss = 0.19230776\n",
      "epoch 41: w = 2.231, loss = 0.19230767\n",
      "epoch 42: w = 2.231, loss = 0.19230773\n",
      "epoch 43: w = 2.231, loss = 0.19230767\n",
      "epoch 44: w = 2.231, loss = 0.19230767\n",
      "epoch 45: w = 2.231, loss = 0.19230767\n",
      "epoch 46: w = 2.231, loss = 0.19230767\n",
      "epoch 47: w = 2.231, loss = 0.19230767\n",
      "epoch 48: w = 2.231, loss = 0.19230767\n",
      "epoch 49: w = 2.231, loss = 0.19230767\n",
      "epoch 50: w = 2.231, loss = 0.19230767\n",
      "epoch 51: w = 2.231, loss = 0.19230767\n",
      "epoch 52: w = 2.231, loss = 0.19230767\n",
      "epoch 53: w = 2.231, loss = 0.19230767\n",
      "epoch 54: w = 2.231, loss = 0.19230767\n",
      "epoch 55: w = 2.231, loss = 0.19230767\n",
      "epoch 56: w = 2.231, loss = 0.19230767\n",
      "epoch 57: w = 2.231, loss = 0.19230767\n",
      "epoch 58: w = 2.231, loss = 0.19230767\n",
      "epoch 59: w = 2.231, loss = 0.19230767\n",
      "epoch 60: w = 2.231, loss = 0.19230767\n",
      "epoch 61: w = 2.231, loss = 0.19230767\n",
      "epoch 62: w = 2.231, loss = 0.19230767\n",
      "epoch 63: w = 2.231, loss = 0.19230767\n",
      "epoch 64: w = 2.231, loss = 0.19230767\n",
      "epoch 65: w = 2.231, loss = 0.19230767\n",
      "epoch 66: w = 2.231, loss = 0.19230767\n",
      "epoch 67: w = 2.231, loss = 0.19230767\n",
      "epoch 68: w = 2.231, loss = 0.19230767\n",
      "epoch 69: w = 2.231, loss = 0.19230767\n",
      "epoch 70: w = 2.231, loss = 0.19230767\n",
      "epoch 71: w = 2.231, loss = 0.19230767\n",
      "epoch 72: w = 2.231, loss = 0.19230767\n",
      "epoch 73: w = 2.231, loss = 0.19230767\n",
      "epoch 74: w = 2.231, loss = 0.19230767\n",
      "epoch 75: w = 2.231, loss = 0.19230767\n",
      "epoch 76: w = 2.231, loss = 0.19230767\n",
      "epoch 77: w = 2.231, loss = 0.19230767\n",
      "epoch 78: w = 2.231, loss = 0.19230767\n",
      "epoch 79: w = 2.231, loss = 0.19230767\n",
      "epoch 80: w = 2.231, loss = 0.19230767\n",
      "epoch 81: w = 2.231, loss = 0.19230767\n",
      "epoch 82: w = 2.231, loss = 0.19230767\n",
      "epoch 83: w = 2.231, loss = 0.19230767\n",
      "epoch 84: w = 2.231, loss = 0.19230767\n",
      "epoch 85: w = 2.231, loss = 0.19230767\n",
      "epoch 86: w = 2.231, loss = 0.19230767\n",
      "epoch 87: w = 2.231, loss = 0.19230767\n",
      "epoch 88: w = 2.231, loss = 0.19230767\n",
      "epoch 89: w = 2.231, loss = 0.19230767\n",
      "epoch 90: w = 2.231, loss = 0.19230767\n",
      "epoch 91: w = 2.231, loss = 0.19230767\n",
      "epoch 92: w = 2.231, loss = 0.19230767\n",
      "epoch 93: w = 2.231, loss = 0.19230767\n",
      "epoch 94: w = 2.231, loss = 0.19230767\n",
      "epoch 95: w = 2.231, loss = 0.19230767\n",
      "epoch 96: w = 2.231, loss = 0.19230767\n",
      "epoch 97: w = 2.231, loss = 0.19230767\n",
      "epoch 98: w = 2.231, loss = 0.19230767\n",
      "epoch 99: w = 2.231, loss = 0.19230767\n",
      "epoch 100: w = 2.231, loss = 0.19230767\n",
      "Prediction after training: f(5) =  11.154\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float32)\n",
    "# y = torch.tensor([2, 4, 6, 8, 10, 12], dtype=torch.float32)\n",
    "y = torch.tensor([3, 5, 7, 9, 11, 13], dtype=torch.float32)\n",
    "\n",
    "# init weight\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# set up loss function as mean square error\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y) ** 2).mean()\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # perdiction = forward pass\n",
    "    y_pred = forward(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # calculate dl/dw\n",
    "    l.backward() \n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "    # zero gradients，要記得歸零每次運算的 gradients，否則會累加\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719d2025-8f50-44c1-bde5-e21708b3e21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True) tensor(0., requires_grad=True)\n",
      "epoch 1: w = 0.677, b = 0.160, loss = 75.66666412\n",
      "epoch 2: w = 1.137, b = 0.269, loss = 35.04683304\n",
      "epoch 3: w = 1.450, b = 0.344, loss = 16.24656868\n",
      "epoch 4: w = 1.663, b = 0.396, loss = 7.54505348\n",
      "epoch 5: w = 1.807, b = 0.432, loss = 3.51754498\n",
      "epoch 6: w = 1.905, b = 0.457, loss = 1.65330660\n",
      "epoch 7: w = 1.972, b = 0.474, loss = 0.79029655\n",
      "epoch 8: w = 2.017, b = 0.487, loss = 0.39068758\n",
      "epoch 9: w = 2.048, b = 0.496, loss = 0.20555431\n",
      "epoch 10: w = 2.069, b = 0.502, loss = 0.11968931\n",
      "epoch 11: w = 2.083, b = 0.507, loss = 0.07976940\n",
      "epoch 12: w = 2.092, b = 0.512, loss = 0.06111584\n",
      "epoch 13: w = 2.098, b = 0.515, loss = 0.05230607\n",
      "epoch 14: w = 2.102, b = 0.518, loss = 0.04805367\n",
      "epoch 15: w = 2.105, b = 0.520, loss = 0.04591171\n",
      "epoch 16: w = 2.107, b = 0.522, loss = 0.04474789\n",
      "epoch 17: w = 2.108, b = 0.524, loss = 0.04403807\n",
      "epoch 18: w = 2.108, b = 0.526, loss = 0.04353959\n",
      "epoch 19: w = 2.109, b = 0.528, loss = 0.04314018\n",
      "epoch 20: w = 2.109, b = 0.530, loss = 0.04278783\n",
      "epoch 21: w = 2.109, b = 0.532, loss = 0.04245850\n",
      "epoch 22: w = 2.108, b = 0.534, loss = 0.04214104\n",
      "epoch 23: w = 2.108, b = 0.535, loss = 0.04183026\n",
      "epoch 24: w = 2.108, b = 0.537, loss = 0.04152365\n",
      "epoch 25: w = 2.108, b = 0.539, loss = 0.04122035\n",
      "epoch 26: w = 2.107, b = 0.541, loss = 0.04091972\n",
      "epoch 27: w = 2.107, b = 0.542, loss = 0.04062142\n",
      "epoch 28: w = 2.106, b = 0.544, loss = 0.04032538\n",
      "epoch 29: w = 2.106, b = 0.546, loss = 0.04003158\n",
      "epoch 30: w = 2.106, b = 0.547, loss = 0.03973984\n",
      "epoch 31: w = 2.105, b = 0.549, loss = 0.03945034\n",
      "epoch 32: w = 2.105, b = 0.551, loss = 0.03916293\n",
      "epoch 33: w = 2.105, b = 0.552, loss = 0.03887763\n",
      "epoch 34: w = 2.104, b = 0.554, loss = 0.03859432\n",
      "epoch 35: w = 2.104, b = 0.555, loss = 0.03831321\n",
      "epoch 36: w = 2.103, b = 0.557, loss = 0.03803397\n",
      "epoch 37: w = 2.103, b = 0.559, loss = 0.03775693\n",
      "epoch 38: w = 2.103, b = 0.560, loss = 0.03748185\n",
      "epoch 39: w = 2.102, b = 0.562, loss = 0.03720878\n",
      "epoch 40: w = 2.102, b = 0.563, loss = 0.03693770\n",
      "epoch 41: w = 2.102, b = 0.565, loss = 0.03666859\n",
      "epoch 42: w = 2.101, b = 0.567, loss = 0.03640141\n",
      "epoch 43: w = 2.101, b = 0.568, loss = 0.03613626\n",
      "epoch 44: w = 2.100, b = 0.570, loss = 0.03587291\n",
      "epoch 45: w = 2.100, b = 0.571, loss = 0.03561158\n",
      "epoch 46: w = 2.100, b = 0.573, loss = 0.03535208\n",
      "epoch 47: w = 2.099, b = 0.575, loss = 0.03509462\n",
      "epoch 48: w = 2.099, b = 0.576, loss = 0.03483889\n",
      "epoch 49: w = 2.099, b = 0.578, loss = 0.03458511\n",
      "epoch 50: w = 2.098, b = 0.579, loss = 0.03433305\n",
      "epoch 51: w = 2.098, b = 0.581, loss = 0.03408297\n",
      "epoch 52: w = 2.098, b = 0.582, loss = 0.03383461\n",
      "epoch 53: w = 2.097, b = 0.584, loss = 0.03358815\n",
      "epoch 54: w = 2.097, b = 0.585, loss = 0.03334347\n",
      "epoch 55: w = 2.097, b = 0.587, loss = 0.03310056\n",
      "epoch 56: w = 2.096, b = 0.588, loss = 0.03285938\n",
      "epoch 57: w = 2.096, b = 0.590, loss = 0.03261999\n",
      "epoch 58: w = 2.095, b = 0.591, loss = 0.03238234\n",
      "epoch 59: w = 2.095, b = 0.593, loss = 0.03214635\n",
      "epoch 60: w = 2.095, b = 0.594, loss = 0.03191222\n",
      "epoch 61: w = 2.094, b = 0.596, loss = 0.03167970\n",
      "epoch 62: w = 2.094, b = 0.597, loss = 0.03144893\n",
      "epoch 63: w = 2.094, b = 0.599, loss = 0.03121981\n",
      "epoch 64: w = 2.093, b = 0.600, loss = 0.03099231\n",
      "epoch 65: w = 2.093, b = 0.602, loss = 0.03076655\n",
      "epoch 66: w = 2.093, b = 0.603, loss = 0.03054236\n",
      "epoch 67: w = 2.092, b = 0.605, loss = 0.03031980\n",
      "epoch 68: w = 2.092, b = 0.606, loss = 0.03009893\n",
      "epoch 69: w = 2.092, b = 0.607, loss = 0.02987965\n",
      "epoch 70: w = 2.091, b = 0.609, loss = 0.02966199\n",
      "epoch 71: w = 2.091, b = 0.610, loss = 0.02944585\n",
      "epoch 72: w = 2.091, b = 0.612, loss = 0.02923131\n",
      "epoch 73: w = 2.090, b = 0.613, loss = 0.02901840\n",
      "epoch 74: w = 2.090, b = 0.614, loss = 0.02880699\n",
      "epoch 75: w = 2.090, b = 0.616, loss = 0.02859704\n",
      "epoch 76: w = 2.089, b = 0.617, loss = 0.02838871\n",
      "epoch 77: w = 2.089, b = 0.619, loss = 0.02818186\n",
      "epoch 78: w = 2.089, b = 0.620, loss = 0.02797660\n",
      "epoch 79: w = 2.088, b = 0.621, loss = 0.02777275\n",
      "epoch 80: w = 2.088, b = 0.623, loss = 0.02757046\n",
      "epoch 81: w = 2.088, b = 0.624, loss = 0.02736953\n",
      "epoch 82: w = 2.087, b = 0.626, loss = 0.02717011\n",
      "epoch 83: w = 2.087, b = 0.627, loss = 0.02697217\n",
      "epoch 84: w = 2.087, b = 0.628, loss = 0.02677570\n",
      "epoch 85: w = 2.086, b = 0.630, loss = 0.02658056\n",
      "epoch 86: w = 2.086, b = 0.631, loss = 0.02638700\n",
      "epoch 87: w = 2.086, b = 0.632, loss = 0.02619471\n",
      "epoch 88: w = 2.086, b = 0.634, loss = 0.02600388\n",
      "epoch 89: w = 2.085, b = 0.635, loss = 0.02581440\n",
      "epoch 90: w = 2.085, b = 0.636, loss = 0.02562633\n",
      "epoch 91: w = 2.085, b = 0.638, loss = 0.02543960\n",
      "epoch 92: w = 2.084, b = 0.639, loss = 0.02525429\n",
      "epoch 93: w = 2.084, b = 0.640, loss = 0.02507029\n",
      "epoch 94: w = 2.084, b = 0.642, loss = 0.02488763\n",
      "epoch 95: w = 2.083, b = 0.643, loss = 0.02470635\n",
      "epoch 96: w = 2.083, b = 0.644, loss = 0.02452634\n",
      "epoch 97: w = 2.083, b = 0.646, loss = 0.02434765\n",
      "epoch 98: w = 2.082, b = 0.647, loss = 0.02417027\n",
      "epoch 99: w = 2.082, b = 0.648, loss = 0.02399413\n",
      "epoch 100: w = 2.082, b = 0.649, loss = 0.02381934\n",
      "Prediction after training: f(5) =  11.059\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float32)\n",
    "# y = torch.tensor([2, 4, 6, 8, 10, 12], dtype=torch.float32)\n",
    "y = torch.tensor([3, 5, 7, 9, 11, 13], dtype=torch.float32)\n",
    "\n",
    "# init weight\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "print(w, b)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "# set up loss function as mean square error\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y) ** 2).mean()\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # perdiction = forward pass\n",
    "    y_pred = forward(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # calculate dl/dw\n",
    "    l.backward() \n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, b = {b:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "    # zero gradients，要記得歸零每次運算的 gradients，否則會累加\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ff0be-63ca-449a-9647-21b1edd19e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
