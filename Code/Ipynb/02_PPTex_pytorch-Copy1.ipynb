{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa2bc5d-846f-4dc1-b4ec-4efa9f3aeb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.066, b = -0.449, loss = 35.34966660\n",
      "epoch 2: w = 1.381, b = -0.375, loss = 16.36580276\n",
      "epoch 3: w = 1.595, b = -0.324, loss = 7.57946444\n",
      "epoch 4: w = 1.740, b = -0.289, loss = 3.51284957\n",
      "epoch 5: w = 1.839, b = -0.265, loss = 1.63066328\n",
      "epoch 6: w = 1.907, b = -0.248, loss = 0.75949448\n",
      "epoch 7: w = 1.952, b = -0.237, loss = 0.35625717\n",
      "epoch 8: w = 1.983, b = -0.229, loss = 0.16959269\n",
      "epoch 9: w = 2.004, b = -0.223, loss = 0.08316402\n",
      "epoch 10: w = 2.019, b = -0.219, loss = 0.04312846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[1], [2], [3], [4], [5], [6]], dtype=torch.float32) \n",
    "y = torch.tensor([[2], [4], [6], [8], [10], [12]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = model(x)\n",
    "    l = loss(y_pred, y)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "    print(f'epoch {epoch + 1}: w = {w[0][0]:.3f}, b = {b[0]:.3f}, loss = {l:.8f}')\n",
    "\n",
    "# print(f'Prediction after training: f(5) = {model(x_test).item(): .3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ea13ac-5b51-4860-9b07-abe072dc6a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.274, b = -0.024, loss = 17.53206635\n",
      "epoch 2: w = 1.496, b = 0.028, loss = 8.11633110\n",
      "epoch 3: w = 1.647, b = 0.062, loss = 3.75843048\n",
      "epoch 4: w = 1.750, b = 0.086, loss = 1.74145067\n",
      "epoch 5: w = 1.820, b = 0.102, loss = 0.80791765\n",
      "epoch 6: w = 1.867, b = 0.112, loss = 0.37583640\n",
      "epoch 7: w = 1.900, b = 0.119, loss = 0.17584199\n",
      "epoch 8: w = 1.922, b = 0.124, loss = 0.08326526\n",
      "epoch 9: w = 1.937, b = 0.127, loss = 0.04040405\n",
      "epoch 10: w = 1.947, b = 0.129, loss = 0.02055290\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[1], [2], [3], [4], [5], [6]], dtype=torch.float32) \n",
    "y = torch.tensor([[2], [4], [6], [8], [10], [12]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = model(x)\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "    print(f'epoch {epoch + 1}: w = {w[0][0]:.3f}, b = {b[0]:.3f}, loss = {l:.8f}')\n",
    "\n",
    "# print(f'Prediction after training: f(5) = {model(x_test).item(): .3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7907d75d-498e-4e65-a33d-4649da32be54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<generator object Module.parameters at 0x7f25b11c7300>,\n",
       " Parameter containing:\n",
       " tensor([[1.9471]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = model.parameters()\n",
    "tt, w ,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a1d70c-a667-43b1-8884-f63ec4317226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., requires_grad=True)\n",
      "tensor([0., 0., 0., 0., 0., 0.], grad_fn=<MulBackward0>)\n",
      "Prediction before training: f(5) =  0.000\n",
      "epoch 1: w = 0.607, loss = 60.66666794\n",
      "epoch 2: w = 1.029, loss = 29.44422913\n",
      "epoch 3: w = 1.324, loss = 14.29059505\n",
      "epoch 4: w = 1.529, loss = 6.93586159\n",
      "epoch 5: w = 1.672, loss = 3.36628079\n",
      "epoch 6: w = 1.771, loss = 1.63380527\n",
      "epoch 7: w = 1.841, loss = 0.79295844\n",
      "epoch 8: w = 1.889, loss = 0.38485837\n",
      "epoch 9: w = 1.923, loss = 0.18678898\n",
      "epoch 10: w = 1.946, loss = 0.09065709\n",
      "Prediction after training: f(5) =  9.731\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8, 10, 12], dtype=torch.float32)\n",
    "# y = torch.tensor([3, 5, 7, 9, 11, 13], dtype=torch.float32)\n",
    "\n",
    "# init weight\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "print(w)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5): .3f}')\n",
    "\n",
    "# MSE in pytorch\n",
    "loss = nn.MSELoss()\n",
    "# set stochastic gradient descent as optimizer\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # perdiction = forward pass\n",
    "    y_pred = forward(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # calculate dl/dw\n",
    "    l.backward() \n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "    # zero gradients，要記得歸零每次運算的 gradients，否則會累加\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942f84e1-e1b6-4358-915c-f3cc329e913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "n_samples = x.shape\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "773c6101-0b59-487b-aada-cc1f0822a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1\n",
      "Prediction before training: f(5) =  0.286\n",
      "epoch 1: w = 0.715, b = -0.191, loss = 57.87015915\n",
      "epoch 2: w = 1.118, b = -0.097, loss = 26.78515625\n",
      "epoch 3: w = 1.393, b = -0.033, loss = 12.39807415\n",
      "epoch 4: w = 1.579, b = 0.010, loss = 5.73929214\n",
      "epoch 5: w = 1.706, b = 0.039, loss = 2.65739870\n",
      "epoch 6: w = 1.792, b = 0.059, loss = 1.23099995\n",
      "epoch 7: w = 1.851, b = 0.072, loss = 0.57081169\n",
      "epoch 8: w = 1.891, b = 0.081, loss = 0.26524901\n",
      "epoch 9: w = 1.919, b = 0.087, loss = 0.12381721\n",
      "epoch 10: w = 1.937, b = 0.091, loss = 0.05835093\n",
      "Prediction after training: f(5) =  9.777\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w * x + b\n",
    "# f = 2 * x + 0, we set w as 2, b as 0\n",
    "x = torch.tensor([[1], [2], [3], [4], [5], [6]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8], [10], [12]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item(): .3f}')\n",
    "\n",
    "# MSE in pytorch\n",
    "loss = nn.MSELoss()\n",
    "# set stochastic gradient descent as optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # perdiction = forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    # gradient descent is where calculate gradient and update parameters\n",
    "    # so gradient descent here includes gradients and update weights\n",
    "    # 原本在 Python 的 example 還需要自己建立 Gradient 函式\n",
    "    # gradients = backward pass\n",
    "    l.backward() # calculate dl/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        # print(w, b)\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0]:.3f}, b = {b[0]:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(x_test).item(): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff63c519-a15f-456a-879e-d96ddaee0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim import optimizer\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d77f7fd9-eaf2-4d5e-b306-db2dabb4bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) prepare data\n",
    "feature_numpy, target_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1234)\n",
    "\n",
    "feature = torch.from_numpy(feature_numpy.astype(np.float32))\n",
    "target = torch.from_numpy(target_numpy.astype(np.float32))\n",
    "target = target.view(target.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66fceb87-9e5f-415c-bf3b-3bae34dc6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        # define layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegression(n_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c44eb99e-0fb1-47ca-aa9c-83cb8b0d6a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        # define layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c038038-08d7-4b4d-b7da-0e5a90af2fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9581e-23,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [-6.2187e-01, -5.4146e-01],\n",
      "        [ 6.8385e-01, -3.7003e-01],\n",
      "        [ 2.0695e-01, -2.0057e-01]])\n",
      "tensor([[-0.7034,  0.2717, -0.6162, -0.5686, -0.4672],\n",
      "        [-0.7034,  0.2717, -0.6162, -0.5686, -0.4672],\n",
      "        [-0.1283,  0.3612, -0.8446, -0.9473, -0.3334],\n",
      "        [-0.7743, -0.2660, -0.2028, -0.2143, -0.9444],\n",
      "        [-0.6734,  0.0686, -0.4762, -0.4671, -0.6419]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.4184, -0.5817],\n",
      "        [-0.5400,  0.4550],\n",
      "        [ 0.5136, -0.1681],\n",
      "        [ 0.5529,  0.0642],\n",
      "        [-0.5129,  0.3420]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(2, 5)\n",
    "x = torch.empty(4, 2)\n",
    "print(x)\n",
    "print(model(x))\n",
    "print(model.linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d94ea-13ce-4711-a4cd-f899d2c21e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
